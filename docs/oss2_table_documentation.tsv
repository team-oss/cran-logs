analysis	run_order	Getting Data	Processing Data	Github	Notes	input	output
pypi	1	01-get_pypi_simple.R			Use http://pypi.org/simple/ to get a list of all the packages listed on pypi.		./data/oss2/original/pypi/pypi_simple/2019-01-23-pypi_simple.html'
pypi	2	02-get_pkg_html_simple.R			"We get the URL for each of the packages listed in pypi.org/simple, and save the page to a html file that we can parse."	./data/oss2/original/pypi/pypi_simple/2019-01-23-pypi_simple.html'	./data/oss2/original/pypi/pypi_simple/simple_pkg_htmls/'
pypi	3		01-pypi_simple_packages.R		"Counts the number of URLS obtained by pypi.org/simple (conversly, the number of packages on PyPI) -- on January 23, 2019"	"./data/oss2/original/pypi/pypi_simple/2019-01-23-pypi_simple.html',"	
pypi	4		02-pypi_simple_latest_src_dl_url.R		"Takes the last package version listed in each of the package HTML pages. We capture this link in order to get the ""latest"" version of the package listed in pypi.org/simple."	"./data/oss2/original/pypi/pypi_simple/simple_pkg_htmls',
'./data/oss2/original/pypi/pypi_simple/2019-01-23-pypi_simple.html'"	./data/oss2/processed/pypi/simple_url_src_paths.csv'
pypi	5	03-01-get_pkg_source_simple.R			"The dataset from `02-pypi_simple_latest_src_dl_url.R` only returns the URL to download the package,
this script then goes through each of the URLs and saves it into the corresponding download location (which is found in the dataset)."	./data/oss2/processed/pypi/simple_url_src_paths.csv'	./data/oss2/original/pypi/pypi_simple/simple_pkg_src/'
pypi	6		03-01-downloaded_src_simple_metadata.py		"the `03-xx` series of script all process the metadata from the downloaded package source files.
We first use the python `pkginfo` package to introspect each of the downloaded packages. We accounted for `.whl`, `.gz`, `.zip`, `.egg`, `.bz2`, and `.tgz` extensions.
The corresponding function within pkginfo is used for each of the various file extensions, and we save the reponse to a column in our dataset.
This script takes a long time to run (~20-30 minutes), that is why these steps are broken up into multiple parts.
This script only saves the reponse from `pkginfo` into a column.
These python scripts save out data in both `csv` and `pickle` formats, the `csv` is really there as a convenience, but all the binary information will be lost.
The `pickle` format is what is actually used between scripts."	"./data/oss2/original/pypi/pypi_simple/simple_pkg_src/',
'./data/oss2/processed/pypi/simple_url_src_paths.csv'"	"./data/oss2/processed/working/pypi/simple_downloaded_pkginfo_attr.csv',
'./data/oss2/processed/working/pypi/simple_downloaded_pkginfo_attr.pickle'"
pypi	7		03-02-downloaded_src_simple_metadata.py		"Information about each package is stored in a python object (from pkginfo). Here we take all the object attributes (stored as a Python dictionary) and converrt it into a dataframe object.
We may not have gotten every bit of information stored in the object, but it captures all the information we want in this project.
There is a renaming of the ""name"" variable to ""name_pypi"" here, since one of the attributes is also called ""name"".
It's important here that you use the ""name_pypi"" as the primary key moving forward, since the ""name"" from the attribute does not always match what was captured from PyPI."	./data/oss2/processed/working/pypi/simple_downloaded_pkginfo_attr.pickle'	"./data/oss2/processed/working/pypi/parsed_pkg_attributes.csv',
'./data/oss2/processed/working/pypi/parsed_pkg_attributes.pickle'"
pypi	8		03-03-parse_production_ready.py		"Development status is captured by the classifier variable, which stores a list of strings of various other ""classifications"" for the package.
Here we extract/parse out the development status string from the classifier variable.
The goal is to perform an analysis on ""production/stable"" and ""mature"" packages."	./data/oss2/processed/working/pypi/parsed_pkg_attributes.pickle'	"./data/oss2/processed/working/pypi/production_ready_first_pass.pickle',
'./data/oss2/processed/working/pypi/production_ready_first_pass.csv'"
pypi	9		04-second_pass_production_ready.py		"Since we looked at the ""latest"" source file when we were getting information from pypi.org/simple, not all packages that were ""production ready"" were marked as such.
For example, the source file we downloaded for `pandas` was an alpha release, but we know it is a ""production ready"" package
Takes the packages that were not already marked as production/stable or mature, and builds the pip download command to download source packages directly from pip."	./data/oss2/processed/working/pypi/production_ready_first_pass.pickle'	"./data/oss2/processed/working/pypi/non_production_ready_pip_download.pickle',
'./data/oss2/processed/working/pypi/non_production_ready_pip_download.csv'"
pypi	10	03-02-get_pkg_source_pip.py			"Runs the pip download command to download the latest pip installable package source.
This was to capture package sources downloaded from 03-01 that were potentially alpha or other ""non production ready"" releases.
If the analysis were to be redone, we would use this method directly (i.e., use pip download instead of manually downloading the ""latest"" source).
This means that `03-01-get_pkg_source_simple.R` to `03-03-parse_production_ready.py` would not need to be run,
and we would capture the pip downloads directly."	./data/oss2/processed/working/pypi/non_production_ready_pip_download.pickle'	
pypi	11		05-01-noprod-downloaded_src_simple_metadata.py		"these scripts all follow the same process as the 03-0x counterparts.
Since the code and functions were not setup to be a python module,
any changes in the 05-0x set of scripts need to be manually changed in the 03-0x set of scripts, or vice versa.

This script introspects the downloaded sources (this time form pip download) and extracts the pkginfo."	./data/oss2/processed/working/pypi/non_production_ready_pip_download.pickle'	"./data/oss2/processed/working/pypi/simple_downloaded_pkginfo_attr_noprod.csv',
'./data/oss2/processed/working/pypi/simple_downloaded_pkginfo_attr_noprod.pickle'
"
pypi	12		05-02-noprod-downloaded_src_simple_metadata.py		Take the object from pkginfo and pivot the attributes to a dataframe	./data/oss2/processed/working/pypi/simple_downloaded_pkginfo_attr_noprod.pickle'	"./data/oss2/processed/working/pypi/parsed_pkg_attributes_noprod.csv',
'./data/oss2/processed/working/pypi/parsed_pkg_attributes_noprod.pickle'"
pypi	13		05-03-noprod-parse_production_ready.py		Clean up the development status from the classifier variable	./data/oss2/processed/working/pypi/parsed_pkg_attributes_noprod.pickle'	"./data/oss2/processed/working/pypi/production_ready_noprod.pickle',
'./data/oss2/processed/working/pypi/production_ready_noprod.csv'"
pypi	14		06-combine_fpass_noprod.py		"Since we collected ""production ready"" packages in multiple ways, this script combines the datasets so we have a single dataset we can use to filter ""production ready"" status.
We filter out dataset here for those packages that are ""production/stable"" and ""mature"" as defined by the deveopment status from the classifier variable."	"./data/oss2/processed/working/pypi/production_ready_noprod.pickle',
'./data/oss2/processed/working/pypi/production_ready_first_pass.pickle'"	"./data/oss2/processed/working/pypi/production_ready.pickle',
'./data/oss2/processed/working/pypi/production_ready.csv'"
pypi	15	04-licenses.R			"Make sure you have the LIBRARIES_IO_API_KEY defined in your r environment.

Takes the ""production ready"" packages and uses the libraries.io API to get more infomration from the each package.
We are just saving the API REST GET reponse here that will be parsed in the next step.
Because the license field of the metadata from pkginfo is all user reported, there are 7000+ unique values put in for license.
We use libraries.io here to get a more standardized list of licenses."	./data/oss2/processed/working/pypi/production_ready.csv'	./data/oss2/original/pypi/libraries.io'
pypi	16		07-01-parse_librariesio_licenses.R		"Parse the GET response from libraries.io to get the 'normalized_licenses' value.
Some packages have multiple licenses listed (some have up to 4).
The first license was used as the license from libraries.io (saved as the column `l`)"	"./data/oss2/processed/working/pypi/production_ready.csv',
'./data/oss2/original/pypi/libraries.io'"	./data/oss2/processed/pypi/librariesio_licenses.RDS'
pypi	17		07-02-osi_approved.R		"There is a master list of OSI approved licenses that is used in this step.
It helps keep track of all the OSI licenses and the ways they can be typed in a license field.

If there is a license missing from the libraries.io service, we fill in the missing license from the ""license"" pkginfo metadata.
We didn't use the license from pkginfo directly, becuase all the information is user input, and there were too many unique license values to account for.

Once we have all the license infomation, we mark each package as having an OSI-approved license or not"	"""./data/oss2/original/osi_licenses_all_projects.csv"",
'./data/oss2/processed/pypi/librariesio_licenses.RDS'"	./data/oss2/processed/pypi/osi_approved.RDS'
pypi	18		08-combine_before_gh.R		"Combine the information about ""production ready"" status with ""osi approved license"" to get a final set of packages that we will use to pull from github.
Once we have the production-ready-osi-approved packages, we then use `home_page` column to potentially get a Github URL,
if no github url was provided in the `home_page` column, we used the `download_url` column.
This was similar to looking at the URL and Bug Report information in the CRAN analysis.
Some github URLs were just the username, so we created the github slug by appending the python package name to create the user/repo slug.

We end up with a dataset of github slugs (which we can use to clone information) that are production ready with OSI approved licenses"	"./data/oss2/processed/working/pypi/production_ready.csv',
'./data/oss2/processed/pypi/osi_approved.RDS'
"	./data/oss2/processed/pypi/prod_osi_gh.RDS'
pypi	19			01-04-clone_pypi.R	"Clones the github projects from the parsed github slug (user/repo).
Not all slugs were valid (i.e., not all github clone urls were valid).
This is because some repositories do not exist anymore, they could've been renamed, and sometimes the user (i.e., owner) does not exist anymore."	./data/oss2/processed/pypi/prod_osi_gh.RDS'	